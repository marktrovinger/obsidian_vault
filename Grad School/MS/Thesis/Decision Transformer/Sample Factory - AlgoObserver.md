## Debugging - General Tips

- Uses `--serial_mode = True`
- Uses the `VizDoom Basic` debug profile in `launch.json` 

## Experiment #1 - ViZDoom Basic
- env: ViZDoom Basic
- timesteps: 10,000
- breakpoint: `train_vizdoom.py`, line 45
- parameters: 
````
python -m sf_examples.vizdoom.train_vizdoom --env=doom_basic --experiment=DoomBasic --train_dir=./train_dir --num_workers=16 --num_envs_per_worker=10 --train_for_env_steps=10000 --serial_mode=True
````

- In `train_vizdoom.py`:
	- parses the config file, calls `run_rl` in `train.py`, which inits the `runner` object and kicks off the experiment, it appears that `AlgoObserver` might go here, after init
- In `runner.py`:
	- In `run()`:
		- Executes the event loop for training/testing
		- Checks to see if the event loop was interrupted
		- Stops the `emit()` function
			- Not sure what this does yet, seems to send a stop signal

## Experiment #2 - ViZDoom Basic
- env: Same as #1
- timesteps: same as #1
- breakpoint: `rollout_worker.py`, line 274
- `RolloutWorker` seems to be where data is collected from the environment
-  parameters:
```
python -m sf_examples.vizdoom.train_vizdoom --env=doom_basic --experiment=DoomBasic --train_dir=./train_dir --num_workers=16 --num_envs_per_worker=10 --train_for_env_steps=10000 --serial_mode=True
```
- In `advance_rollouts()`:
	- Looking at the stack trace, we might be able to access the data needed through 
	```
	runner.traj_tensors_torch["cpu"]["obs"]["data"]
	runner.traj_tensors_torch["cpu"]["actions"]
	runner.traj_tensors_torch["cpu"]["rewards"]
	runner.traj_tensors_torch["cpu"]["dones"]
	```
	- These are `torch` tensors, and would need to be converted (maybe?) to `numpy` tensors
		- If so `.detach().cpu().numpy()` to make sure we aren't copying any gradients, which shouldn't be there for obs (looking at this now, since they are already on CPU, we might just be able to use `.numpy()`)

## AlgoObserver Ideas
- Need to call `runner.register_observer()` before calling `run_rl()`
- `population_based_training.py` has a (potentially) good example of more of the functionality of `AlgoObserver`, including what can be accessed from the `runner` object
- Use a clone of SB3's `ReplayBuffer`, even if we have to put the functionality out and into a different file or import `ReplayBuffer`, as the conda env has both installed
- Function `on_init()`:
	- Usual variable initialization stuff
- Function `on_start()`:
	- Init variables that rely on other variables, such as the environment and potentially model related variables
- Function `on_training_step()`:
	- If we are sampling, check if we want to sample
	- If so, add the trajectory to the replay buffer
		- We need to figure out how this works with async
			- Does each `RolloutWorker` run for the total number of training steps?
			- Can the `runner` see all of the data generated by all of the `RolloutWorkers`?
	- We could compare rewards from all of the `RolloutWorkers`, selecting the trajectory that has the highest rewards 
- Function `on_stop()`:
	- Write the buffer to disk, the code from `cleanrl` written by Perusha should be a drop-in
	- Print some statistics about the dataset to make sure that we are generating the correct data

## Code Sketch
```
from sample_factory.algo.runners.runner import AlgoObserver, Runner
from stable_baselines3.common.buffers import ReplayBuffer
import numpy as np

class ReplayBufferObserver(AlgoObserver):
	def __init__(self):
		super(ReplayBufferObserver)
		self.replay_buffer = None

	def write_rb_to_disk(self):
		# copied from cleanrl
		np.savez_compressed(f"{data_dir}observations_{file_idx}", observations=np.array(states))
    np.savez_compressed(f"{data_dir}actions_{file_idx}", actions=np.array(actions))
    np.savez_compressed(f"{data_dir}rewards_{file_idx}", rewards=np.array(rewards))
    np.savez_compressed(f"{data_dir}dones_{file_idx}", dones=np.array(dones))
    np.savez_compressed(f"{data_dir}steps_{file_idx}", steps=np.array(steps).reshape(len(steps),1))
    np.savez_compressed(f"{data_dir}gblsteps_{file_idx}", gblsteps=np.array(gblsteps).reshape(len(gblsteps),1))
	
	def on_start(self):
		self.replay_buffer = ReplayBuffer(size, env.observation_space.shape, env.action_space.shape)

	def on_training_step(self, runner):
		if num_steps > 1:
			replay_buffer.add(obs, action, rewards, done)

	def on_stop(self):
		# write any statistics we need to logs or otherwise
		...
		# write replay to disk
		self.write_rb_to_disk()
```

## Experiment #3 - ViZDoom Basic
- Same parameters as #1 and #2
- Used a very basic sketch of `ReplayBufferObserver`, to pause on training step
- Observations:
	- `on_training_step` didn't stop where I thought it would; the `RolloutWorkers` do their thing  for quite awhile before it hits this function
	- In an async setting, the `PolicyWorkers` will churn through collected experiences and then start to get to timeout
	- Each `RolloutWorker` has a `trajectories_per_training_iteration` length buffer, so we could collect `n_workers*trajectories_per_training_iteration` trajectories.
	- I'm not sure `ReplayBuffer` will work the way we want, but it is worth a try

### Results
- It seems like using the `RolloutWorker` should work; I was able to collect samples using the `buffer_mgr` object's `traj_from_torch`, which give us access to the `obs, actions, rewards,` and `dones`. 
- Simply writing the buffers out didn't work; the resulting data is a list of `torch` tensors, will discuss with Perusha how we want to handle this going forward

### Future Experiment Goals
- Fix writing buffers to disk so that we are writing the correct data in the correct form.
- Test with async to ensure we are not negatively impacting overall performance.


## Experiment #4 - ViZDoom Basic (Still)
- Same parameters as #1 and #2, except for `rollout` length to 128, to match the discussion with Perusha
### Results
- Running in async mode, it generated more than 250GB of data before I killed it
- Need to change from sampling from every `RolloutWorker`, to a different `RolloutWorker` every training step
	- This currently works, but generates very little data
		- This might be a quirk of sync mode, test with async
		- More likely due to the folder already existing for the training experiment
- On 100M timesteps, length of the `ReplayBuffer` is `2023`, which means we have (I think) `647,360` records
	- This uses a sampling rate of 1 buffer per training iteration



## Experiment #5 - VizDoom HGS
- Same parameters as #1 and #2, except for `rollout` length to 128, to match the discussion with Perusha, and environment is set to `doom_health_gathering_supreme`
- Testing:
	- At the end of the training, 10% of the overall data is being written to disk, instead of the entire `ReplayBuffer`
### Results
- `TypeError: unhashable type: 'TensorDict'`
- Also, `key` is listed as a tuple, when it really shouldn't be
### Next Steps:
- Fix error, need to look at what type comes from sampling

## Experiment #6 - ViZDoom HGS
- Same parameters as #1 and #2, except for `rollout` length to 128, to match the discussion with Perusha, and environment is set to `doom_health_gathering_supreme`
- Running without the `AlgoObserver`, to measure the correct variable
- Testing
	- What does the performance curves look like when `rollout=128`? How much does this hurt agent performance?
### Results
- Very promising!
- https://wandb.ai/doom_decision_transformer/doom_rollout_128/runs/doom_hgs_rollout_128_20230429_222750_272485?workspace=user-marktrovinger

## Experiment #7 - ViZDoom Basic
- Basic environment, testing to see if sampling works
- Running with `AlgoObserver` running, so I can see if the sampling is going to work

### Results
- Failed, due to files eating disk space

### Next Steps
- Determine what is the size of the `ReplayBuffer` when the training stops, and determine how much data we actually want to save, and how to determine that the data is moving with the arrow of time, and is generated from improving policies

## Experiment #8 - ViZDoom HGS
- HGS environment, testing to see if sampling works
- Running with `AlgoObserver` running, so I can see if the sampling is going to work

### Results
- Failed, due to queues filling and entering basically an infinite loop

### Next Steps
- Change to a single sample generated per training iteration and try again
- Move to a shared repo folder so that Perusha can use/work on it as well

## Experiment #9 - ViZDoom HGS
- HGS environment, testing to see if sampling works
- Running with `AlgoObserver` running, so I can see if the sampling is going to work
- Moved to a different folder, in the `doom_decision_transformer` folder instead of the `sample_factory folder`

### Results
- Works in `doom_decision_transformer`

### Next Steps
- Look at only logging when the experiment is near the end, last 10M timesteps or so, collect only expert data 

## Experiment #10 - ViZDoom Basic
- Basic environment, testing to see if sampling works for expert level data collection, using same parameters as first 3 experiments

### Results
- While we know how many environment steps have elapsed, need to figure out how the system keeps track of steps taken vs allowed, kept in the `runner.cfg.train_for_env_steps`
- Writes 4 small-ish files to disk for 10k timesteps (292 MB on disk)
- Also changed functionality of the `ReplayBufferObserver`, to write to the training dir for output

### Next Steps
- Test with greater timesteps, `--serial_mode=False`, and `--rollout=128`, see experiment #11 

## Experiment #11 - ViZDoom Basic
- Basic environment, testing to see if sampling works for expert level data collection, using same parameters as first 3 experiments, testing now with a longer timestep length, async more and a longer rollout
- Making sure that the writing doesn't kill the system
- Questions that need answers:
	- How many files will this generate?
	- Will this affect performance, namely writing to disk?
		- Kill the process/zombify it?

### Results
- How many files will this generate?
	- 6!
	- Each: 1.1 GB on disk
- Will this affect performance, namely writing to disk?
	- Kill the process/zombify it?
		- Doesn't seem to!

### Next Steps
-  Run an experiment with HGS, for 100M timesteps and re-check, see experiment #12

## Experiment #12 - ViZDoom HGS
- HGS environment, testing to see if sampling works for expert level data collection, using same parameters as experiment #6, including WandB
- Making sure that the writing doesn't kill the system
- Questions that need answers:
	- Can the data generated be read for DT?
		- Can we write DT compatible files instead of having a conversion step?
	- Using more compute resources, will this hurt performance?
	- How many files will this generate?
	- Will this affect performance, namely writing to disk?
		- Kill the process/zombify it?

### Results
- https://wandb.ai/doom_decision_transformer/doom_rollout_128/runs/doom_hgs_rollout_128_20230504_183841_721157/overview?workspace=user-marktrovinger
- Generated 63 files at 4.6GB/file
- Length of roughly 1M timesteps

### Next Steps
-  Use the `read_sf_files.py` script to generate DT happy data and test, using a `rtg` of 30, see experiment #13

## Experiment #13 - ViZDoom HGS using DT!
- ~Using the script referenced in experiment #12, generate the necessary files for DT, and then run the `run_dt_atari_v3.py` file to see results~
- Use the script code to write the files directly
- Have this done before the meeting tomorrow, use `nohup` to generate an output file


