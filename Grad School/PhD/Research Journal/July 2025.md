
## July 1, 2025
### MARL+HF
I'm looking at possible approaches taken by others for the automated labeling of trajectory segments, using (ugh), an LLM for SMAC (or really any game based trajectory data).

## July 3, 2025
### Meeting w/Dr. Chen
[[July 3 - Dr. Chen]]

Didn't happen, he was sick.

### MARL+HF
Need to figure out what I want to do about the change to LLM based model.
### Paper Reading Meetings
SMACv2,  [[MADDPG]], and MAPPO. More of a basic overview than a deep dive.

## July 5, 2025
### ViZDoom Wrapper
Have a meeting on Monday, hopefully will have some work done on this project before then, I am so far behind.

## July 7, 2025

### ViZDoom Wrapper
I have a very workable setup, connecting to striker, and I should be able to make great progress while using my laptop as my daily driver. Meeting with Marek today, going to work on potential approaches since Doom uses a server-client arch. There are questions that I think I need to ask Jordan or Marek, which are:
- Client/server arch? Does PZ allow for that, or if not, what would be the best way to implement that? Running each player in it's own thread?
- Do we need (or want) a global state in addition to the individual agent states?

#### Working on:
- Better understanding of how Doom's multiplayer actually works
- Sample Factory has a multi-agent wrapper, looking into how we might use that?
#### Notes, TODOs
- Develop a multi-agent wrapper, using the SF example as a template
- ~~Send to Marek the example used in SF

### Kinova Project
Need to finish my slides today. I would like to record, but that seems unlikely.

## July 9, 2025
### Meeting w/ Dr Chen
Committee:
- Rules for who can be
- Email anyone who would be a good fit
- Chen, Sun, Bethel, other Chen, Swan
- Submit form to apply courses

## July 10, 2025
### Kinova Arm Project
Recorded and submitted both the recording and the slides. Messaged Dr. Chen about that.

## July 11, 2025
### ViZDoom Wrapper
I need to read through what Marek sent me about the approach that SF uses. If he is fine with doing something similar I will work on that both this weekend and next week as well. Notes from Marek's message:
- The content is very similar to the multiplayer examples, and that it should probably be a part of the PZ wrapper.
- Not sure I entirely understand what he is saying here, I will need to look into this more. I should take some notes on the ViZDoomEnvMultiplayer class and see what I discover.

I am also going to read and take notes on the [[PettingZoo]] paper, it might be helpful for both this project and MARL+HF.

### MARL+HF
I am going to read through the [[SMACv2]] paper to see if the reward structure for the agents fits what Dr. Sun is concerned about. PettingZoo seems to indicate that the reward is both a global and a per agent reward, I will investigate and note here which is true. Dr. Sun's concerns are as follows:
- Without ground-truth reward function for each agent, we can let LLM give feedback for each agent rather than use the analytical models
- The involvement of LLM actually aligns better with typical RLFH
My response:
- In this case, the LLM would take trajectory segments as input and provide preference labels for each segment for each agent, correct?

Based on that, it would make sense to look at how to use an LLM for preference labelling, using something like SMAC as the environment. The `og-marl` tool has datasets for both SMAC and v2: https://huggingface.co/datasets/InstaDeepAI/og-marl

## July 13, 2025
### MARL+HF
I would like to message Dr. Sun no later than Tuesday, and I am working through the papers that address preference based learning in MARL settings. I created a folder for these papers in Zotero, under PhD-> MARL+HF.

Both of the papers look promising, and I will be working on reading and understanding them, possibly later today. Notes for a message for Dr. Sun:
- offline datasets for SMAC exist
- papers found that do preference based learning, with an idea of how we might incorporate the corruption methods detailed in that paper

## July 14, 2025
### MARL+HF
Working on notes for papers that might be relevant to this work to send to Dr. Sun, ideally today or tomorrow at the latest. Relevant links:
- Datasets: https://huggingface.co/datasets/InstaDeepAI/og-marl
- Algo implementations: https://github.com/typoverflow/WiseRL
Relevant papers are in the PBL folder of Zotero.

### ViZDoom
Meeting today at 2. Need to work some on what would need to be done to get the wrapper working before that call.

## July 15, 2025

### MARL+HF
Worked on notes for papers sent to Dr. Sun, [[O-MAPL - Offline Multi-agent Preference Learning]], [[Hindsight Preference Learning for Offline Preference-based Reinforcement Learning]] and [[Offline Multi-Agent Preference-Based Reinforcement Learning with Agent-aware Direct Preference Optimization]]

The main areas examined were how suitable the work in question is to further exploration. I should have a decent idea of how any of these works could be used before meeting with Dr. Sun, probably at the end of the month or in August.

### ViZDoom Wrapper
I chickened out of the meeting on Monday, and starting tomorrow, I want to put serious time into that project. The weather looks like it will be rainy enough that I can work inside most of that time, so I should be working on this and MARL+HF, while I have time to do so.

## July 17, 2025
### MARL+HF
Working today on a sketch of how the pipeline will work, using the following ideas:
- Use the basic flow outlined in the [[Robust Reinforcement Learning from Corrupted Human Feedback]] paper
- Use the reward model mentioned in Dr. Sun's proposal?
- Use the preference labeling with a LLM mentioned in [[O-MAPL - Offline Multi-agent Preference Learning]]
- Use the dataset for SMACv2 from https://huggingface.co/datasets/InstaDeepAI/og-marl

Questions that might still need an answer, once the sketch is complete:
- Should the reward model follow the proposal? Dr. Sun is indicating probably not, but if we are able to get agent specific rewards, that might be do-able
- For the LLM labeling: is this something that can be achieved locally?
- Do the authors have the dataset created from the LLM labelling?
- How do the pred rewards from [[Deep Reinforcement Learning from Human Preferences]] interact with the rewards from the env?

TODOs:
- ~~Create initial sketch on the whiteboard, using just blocks and arrows
- ~~Create a more detailed diagram using something like https://app.diagrams.net/
- Notes for [[Deep Reinforcement Learning from Human Preferences]], make sure I understand that whole process, find code if possible


### ViZDoom Wrapper
Pulled myself off the project to work on MARL+HF more.

## July 18, 2025
### Kinova Group
Meeting notes:
- Nothing really interesting


### MARL+HF

### Paper Reading Group
Went well, will continue working on the SMACv2 env. Possibly work on using something like Hierarchical RL or some other approach.

## July 21, 2025
### Robust MARL
I have renamed this section to better reflect the goals of the project. I am working on getting SMACv2 working on my desktop so I can start gathering data about how the rewards and other components are structured. I am also working on getting a better idea of how the mixing network might work, based on the rewards from the agents.

## July 22, 2025
### General
Due to MS deciding that copilot needs to start when VSCode does, I am switching to Neovim full time, despite how painful it will likely be. I am going to keep track of things I need to learn to do in Neovim here:
- Debugging
- Running executables (might just use Tmux)

## July 23, 2025
### General
Switched back to Mac, too easy to get distracted by SM2. Also, too much muscle memory for cmd+* to do basic tasks. Still working on getting a good workflow to replace VSCode, using tmux+neovim.

### Robust MARL

## July 24, 2025
### Robust MARL
Working through some of the practical issues that will come up when implementing the pipeline, and I will update Dr. Sun tomorrow with progress.
Pipeline:
- LLM annotates trajectories, which is used in the mixing network of the reward function
- Rewards are fed to the RL algo $\pi$ along with the obs from the environment, with the reward replaced by the pred reward from the reward function
- $\pi$ takes actions and sends them to the env

Practical Issues:


## July 28, 2025
### Robust MARL
Working on getting a feedback pipeline working, need to determine how fast/slow this process is. Would like to have that done today to send an update to Dr. Sun. If I am able to get it working, I will start on the reward model as diagrammed in the proposal paper.

## July 29, 2025
### General
Just joking about switching to Neovim, I am so much less productive, and I figured out how to turn off the copilot crap. 

### Robust MARL
Messaged Dr. Sun about progress, still working on automating the HF component, should get that working sometime this week. Also starting on a reward model that uses a mixing network on the preference labels.

## July 30, 2025

### Robust MARL
Working on getting an understanding of the underlying environment.

## July 31, 2025
Extracted the agent's health from the observation, working on enemy health right now. The structure is that the first 4 elements (0-3) are movement, and the next fifteen are enemy information, when flattened. I might want to look at unflattening these.

