## June 2, 2025
- [[Robust Reinforcement Learning from Corrupted Human Feedback]] and [[Direct Preference Optimization - Your Language Model is Secretly a Reward Model]] papers analyzed
	- Well, glanced through anyway

Research question refinement, using the idea of robust MARL from HF, how should we approach this?
- Using simulated human preference labels (generated either through a stochastic, myopic, or irrational noise process as mentioned in [[Robust Reinforcement Learning from Corrupted Human Feedback]]) and the reward structure mentioned in the proposal, can we demonstrate that the model is robust to perturbation or poisoning in the feedback data?

Dr. Sun's proposal mentions Mixing functions, I'm not entirely sure what those are?
- Looks like the idea is that the monotonic portion is more important, it increases linearly with the inputs
Also mentioned are "parameterized by neural networks", which just means that we can use a NN as a function approximator.

Meeting with Dr. Chen's group on Friday, probably need to have some progress made on multi-agent Kinova, using the repo I sent him a couple months ago

## June 3, 2025

### Optimization Course
The PFW library has a copy of the textbook used in MIT OCW course 15.093J, I have downloaded the course content and plan to work for at least an hour each day on the course

### MARL+HF
I sent a revised research question to Dr. Sun, probably will need at least a couple more rounds of revision before we proceed. Plan to spend some time thinking about that today. I also added some notes to the two papers referenced in the proposal document.

### Kinova Project
Checking on the status of my ability to add a second robot to the scene. Probably need to examine both the file used in the project to the Allegra Shadow Hand Project.

## June 4, 2025
### MARL+HF
Dr. Sun indicated that QMIX is the algorithm used in the paper. I am working on building up my knowledge of QMIX, both the theoretical background of the algorithm, but also an implementation as well.

### Optimization Course
As the PFW library has the book used in the course, I will pick up the copy tomorrow after my meeting with Adolfo. Starting the course either tomorrow or Friday.

## June 5, 2025
### Optimization Course
Discontinued for now, I think I have too much on my plate to do this.

### MARL+HF
I need to draft a response to Dr. Sun about next step, really need to workshop this before sending.
- Which base MARL algorithm is he referring to?
	- Options: QMIX, MADT
	- He mentions that qmix is used in the proposal
		- The Mixing network in the proposal in equation 3, is where qmix comes in; the mixing function in monotonic, as is `q_total` in qmix
- Starting with the algorithm: We need a robust reward model to learn, what should that look like/be?
	- The reward model should follow equation 3 in the paper, and use the parameterization mentioned in that paper, Dr. Sun didn't indicate that I should be working on something different, so the reward model makes sense.
- Starting with the data: We need a dataset of MA trajectories, since we are using the simulated human feedback (assuming we are) we would need a method to annotate the data, using the methods outlined in [[Robust Reinforcement Learning from Corrupted Human Feedback]]
- Which should we start with? 

### Kinova Project
Starting to detail the differences between the Kinova repo and shadow hand. Thus far:
- Direct (shadow hand) vs. Manager-based (kinova) (from docs)
	- **Manager-based**: The environment is decomposed into individual components (or managers) that handle different aspects of the environment (such as computing observations, applying actions, and applying randomization). The user defines configuration classes for each component and the environment is responsible for coordinating the managers and calling their functions.
		- Doesn't look like there is a manager-based MARL config, we would need to use the direct MARL env as a base class
	- **Direct**: The user defines a single class that implements the entire environment directly without the need for separate managers. This class is responsible for computing observations, applying actions, and computing rewards.
		- Based on PettingZoo's `Parallel`, but doesn't inherit from it or `VectorEnv` from gym
- `--headless` really speeds up training!
- Can only use skrl for true multi-agent training
- Kinova project extends `ReachEnvCfg`, which is a manager-based environment config
	- We would need to create a standalone project
		- Direct MARL
		- Define the task
			- Actions taken by the robots
			- Reward structure
			- Scene creation
				- Some of this can be taken from the Kinova project
			- Observations
				- Uses a `dict`, with `policy` as the key
	- Domain randomization?
		- Shadow hand seems to use this, not really sure what this is for

## June 6, 2025

### ViZDoom PettingZoo
I have a meeting with Marek on Monday, need to prep for that meeting over the weekend.

### MARL+HF
Working on a sketch of how the robust reward model should be implemented, need to discuss with Dr. Sun how the output should be structured, start with a hand-drawn diagram of the possible structure.

### Kinova Project
Meeting today at 11 AM. Meeting notes will go in this section.

#### Meeting Notes
- Leo working on the MA version of the tactile sensors, ordered new sensor due to tear
	- Generated random data for the MA setup, checking runtime, MA was 8-9% slower than single-agent, section of code was $n^3$, now $n^2$
	- GPU laptop should be available
- Abstract submitted to a conference in Boston, Leo can't travel to Boston, and Dr. Sun isn't going either, New England Manipulation https://nems25.github.io/
- Look for Robotics Symposiums near me, Purdue, Michigan, MSU, Ohio State, etc.
- Target ICRA 2026, can also submit to a journal, ICRA will allow for accepted papers that were submitted to a journal
- Multi-agent version of the tactile sensor, using camera for pose estimation, grasp prediction network

## June 8, 2025
### Kinova Project
Leo sent me a couple of files for the multi-agent gripper. The code is kind of a mess, and I'm not sure what it is supposed to do, but I will keep working through it.

### MARL+HF
Started a hand-drawn diagram, I need to get answers to the following questions:
- $\delta$, the perturbation function should be parameterized, what are the inputs and outputs of that network?
- $R(s,a)$ is described as the annotator's bias in the [[Robust Reinforcement Learning from Corrupted Human Feedback]] paper, I think, how does that work?

I want to focus on a more careful reading of the paper today, and taking notes on what I find that can help answer those questions. I looked for a code implementation of the paper, and it doesn't look like one exists, although the author's GitHub repo has something called https://github.com/abukharin3/ERNIE  [[ERNIE]], not sure if that will be relevant or not. I should mention that paper to Dr. Sun after studying it over the next couple of days.

### ViZDoom PettingZoo Wrapper
I have a meeting on Monday with Marek at 2 to discuss this project, I should spend at least an hour on Monday (ideally today) to create some notes and an outline of questions for the meeting. 

## June 9, 2025
### Kinova Project

### Paper Reading Group
Prep for meeting on Friday, work through at least part of the QMIX paper ahead of time, probably need to do that anyway to better understand the monotonic mixing.

### MARL+HF
I want to work on a possible perturbation function, $\delta$, using some single-agent data. This should follow the method detailed in [[Robust Reinforcement Learning from Corrupted Human Feedback]], specifically looking at the data used in that paper, which is the PyBullet data for HalfCheetah, Ant and Hopper.
- Preference labels are generated based on ground truth from the environment, does this mean that the reward is used to determine if the choice is considered preferable?
- The method detailed in the paper, in 5.1, might detail a roadmap for how to implement the multi-agent version of that approach
	- Using stable-baselines3 and pettingZoo instead of RL-Zoo for the training of the algorithms, the docs has a guide for using sb3 and PPO, possibly multiwalker environment?
	- AEC vs Parallel, might be a question for another time
	- For $r^*$, do we just use the ground truth from the environment to start?  
	- $\hat{r}$ is the reward function I need to work on, they use PPO for the policy $\pi$, and the dataset is generated from that policy after 10k steps 

### ViZDoom PettingZoo Wrapper
#### Questions pre-meeting
- AEC vs. Parallel
- Have an updated copy of ViZDoom's repo open on brawler so I can reference if needed (probably will be)
Notes for the meeting will go here:
- Make the env more friendly for LLMs, having LLMs orchestrate lower-level controllers
- Simplified envs might be too simple, maybe make more complex envs
- Bunch of bugs and qol improvements
	- Paper possibly?
	- Adding is simple, handles map transition
	- Full Doom game, full scientific paper
		- Full by the end of the year
- Possibly use SF to complete levels?
- PettingZoo levels
	- Coop for tasks
	- Examples in the repo for multiplayer
	- Doesn't use API, maybe create an API for multi/PettingZoo
#### TODOs for next meeting
- v0.0.1 working for next meeting

## June 10, 2025
### Kinova Project
Messaged Leo about a meeting, he sent a diagram of how the system is supposed to work, will take a look at that later, and try to re-create on my whiteboard.

### MARL+HF
I started some code for the reward function, need to figure out what the inputs would be in the MA case, the single agent case should be easy. Also need to figure out what the output would be as well in the MA case.

### Paper Reading Group
#### Qmix
Working on notes for Friday, meeting at 1 EST.

### ViZDoom Wrapper
Working on a skeleton implementation of the AEC wrapper, also need to work on a very basic test map, maybe built in DM?

## June 11, 2025
Overall, very little accomplished today, will note any work done this evening.

### MARL+HF
Worked on getting MultiWalker working with PPO, need to spend some more time with that, both tonight and tomorrow.

### Kinova Project
Set a meeting with Leo, need to create that meeting.

### ViZDoom Wrapper
Continuing the skeleton, plan to finish tonight, hopefully to finish out the day strong.

## June 12, 2025
Spoiler alert; I didn't get to anything last night.

### MARL+HF
I am working on developing a set of questions that I can ask Dr. Sun about how best to proceed. I think I need something like:
- In the papers mentioned, the reward model $\hat{r}$ is trained asynchronously from the policy $\pi$, with the feedback used to update the weights of $\hat{r}$ every so many epochs, would that training method work in our case?
- The ground truth reward $r^{*}$, should be the reward from the environment, OR should they come from the annotation process?
- The equations are slightly different for the proposal and the original paper, how come?
- The trajectory segments, should they come from the same timesteps, the description of them would indicate yes, but I want to make sure
- $\delta^*$ is referred to as a sparse vector, which makes sense for the single-agent case, but in the MARL case, it should be a matrix of values?
- How does the reward model train initially? Should it use ground truth from the env or random?
- For the perturbation, what are the labels for the network?
- Should we use the alternating optimization mentioned in 3.3?

## June 13, 2025
### MARL+HF
Got the QMIX Docker image working on Brawler, will link to the experiments when they finish. May want to look into a second GPU, if I can run two in that machine. Need to also message Dr. Sun about a meeting for next week.

### ViZDoom Wrapper
Started (finally) on the skeleton today, hope to add more as the day goes on.

### Paper Reading Group
Next meeting will cover [[DreamerV2]].

## June 16, 2025
### ViZDoom PZ Wrapper
Meeting with Marek today, a list of questions I need to ask him will go here:
- Host node, so each agent would need to connect to the host, would the agent 0 be the host? Or could be the spectator?
- Possibly setup a meeting with Jordan, to discuss any problem with her

I'm continuing to work on the PZ wrapper, need to look at how the game spawning works in multiplayer, it will probably need some work to make sure it doesn't spawn more than one instance of the game itself. 

Goal for basic functionality:
- Using the basic deathmatch map, with both controlled by agents, does this sound reasonable for a basic version

Multiplayer environments:
- Cooperative environments?



### MARL+HF
Still working on developing a list of questions I need to ask Dr. Sun. Looking back at the questions posed on June 12, some of them probably need more refinement. Will pose these revised questions below. 
- If we use the SMAC data, which I need to investigate how difficult that would be to generate, do we want to use some of the data corruption techniques mentioned, or try to human annotators (I am partial to using machine annotations

I need to message Dr. Sun about my reduction in hours, so he is able to have realistic expectations for me moving forward, which is now done. Investigating the QMIX run failure, it looks like it was CUDA OOM, but I'm not 100% sure about that.

### Paper Reading
Created a note for [[DreamerV2]].

## June 17, 2025
### MARL+HF
Meeting with Dr. Sun on Friday morning to discuss questions I have and future directions. I'm going to continue revising my questions (and hopefully answering some of them beforehand), and will keep a running list in my notes here.

### ViZDoom PZ
Moved to working locally on my laptop, to ensure I don't have any weirdness if I get disconnected from brawler. Hoping to have a basic implementation working this week, although that might be a bit optimistic.
Ideas:
- SMAC has a PettingZoo wrapper that might be helpful, especially interface between the library and the underlying game
- ASYNC_PLAYER vs. PLAYER
	- ASYNC_PLAYER might be the better choice in a `Parallel_Env`
	- PLAYER might be better for `AEC`

Notes for SMAC
- Structure:
	- `MultiAgentEnv` is the dummy base class 
	- `StarCraft2Env` inherits from the base class
	- `smac_parallel_env` implements the PZ wrapper
- `_launch()` in the base `StarCraft2Env`actually launches the game

## June 18, 2025

### MARL+HF
Continued revising questions, didn't make a lot of progress.

### ViZDoom Wrapper
Worked on a possible structure.

## June 19, 2025
### MARL+HF
Revising questions for Dr. Sun:
- Ground truth: rewards $r^*$ come from the env in the single agent paper, should we do the same, for the same reasons?
- Noise Models: mentioned as NN in the proposal, what should those look like? Do we want a NN for each type or one that can take different forms?
- SA paper compares their method to a baseline of cross-entropy loss, I think that would be appropriate for our case, since it is the difference in labels, but would it work the same? (I think there might be scenarios where it wouldn't work)
- How similar are the Mixing function and QMIX?
- Are they using the same $\pi$ network for both data generation and training the reward model $\hat{r}$?
- Data generation: use MAPPO with SMAC?
- Dataset $D_0$: testing set, do we retain the ground truth reward $r^*$ as the preference label 
- Are the stages for training:
	- Training initial policy $\pi$
	- Dataset $D_0$ generated using $\pi$ initially and then use the noise model $\delta$ for preference labels 
	- Reward model $\hat{r}$ training
	- Train model $\hat{\pi}$ 
- Algorithm:
		

## June 20, 2025
### MARL+HF
Meeting notes:
- $r^*$ : ground truth from the environment
- Noise Model: can be state dependent, softmax to bound
- Mixing: exactly QMIX
- Testing always contains the ground truth, so we should use the same
- We should also use CE
- Input should be collections of trajectory
- Possible values of $m$: no preference, can be a hyperparameter
- Environment: no preference from Dr. Sun
	- Find env that returns agent performance, not just group performance
- Evaluating each agent individually instead of the group as whole
- Do we have $r^*$?
	- Maybe?

Working on generating data, and trying to find answers to the $r^*$ question in SMAC or other MARL environments.


### Kinova Group
Meeting notes:
- Presentation and recording needs to be done before July 10, I will be working on the actual recording
- Use some of the visual artifacts created but not used in the course of the project


### ViZDoom Wrapper
Reward structure for each agent, instead of a global reward for coop tasks?


## June 23, 2025
### MARL+HF
Looking at how to integrate an LLM, since that is the direction that Dr. Sun wants to investigate. Possible ideas will go here:
- 

### ViZDoom Wrapper
Meeting with Marek today at 2, should have a good outline of how the wrapper should work. Need to look at contributing guide to make sure I am following the practices as laid out in that document. I plan to create a GH issue to track work that needs done, what testing should look like, etc.
Notes:
- Each agent will need to connect to the existing game, which should be spawned when the env is created, on localhost: `game.add_game_args("-host 2 -deathmatch +timelimit 1 +sv_spawnfarthest 1 ")`
- Connect: `game.add_game_args("-join 127.0.0.1")` and `game.add_game_args("+name Player2 +colorset 3")`
- The number of agents will dictate the `PlayerN` value, and should be specified either in the config or when invoking?
Meeting Qs:
- Is there a contrib guide?
	- Precommit
	- PRs are not so strict, since we are meeting
	- CICD is setup and working, test should run automatically
- Should I create a GH issue to track against?
- Config file or constructorx
- Agent count as argument


### Kinova Group
Working on slide deck for conference presentation.

## June 24, 2025
### Dreamer v2 Paper
Working on notes, using the questions from the QMIX paper as a reference while reading.

### ViZDoom Wrapper
Idea for a basic implementation: 
- Hosts a local game
- Agents then connect to that game
- Single reward is returned

## June 25, 2025
### Dreamer V2 Paper
Started working on notes in a serious way.
### ViZDoom Wrapper
Working on a diagram that will help define how this wrapper should work. I'm using this diagram to ensure that the approach will meet the requirements and work with the ViZDoom framework correctly.

## June 27, 2025

### ViZDoom Wrapper
Spent some time thinking about how the wrapper would work in relation to the SC2 wrapper. 

### MARL+HF
Not much.

## June 29, 2025
### ViZDoom Wrapper
Meeting with Mark tomorrow, would like to have a good idea of where I stand with the wrapper, also going to message him on Discord to find out which system he uses to build primarily.

## June 30, 2025

### ViZDoom Wrapper
Meeting with Marek today, working on getting a wrapper that can do at least the following:
- Launch an instance of ZDoom
- Connect a player to that instance

There are unanswered questions that I need to formulate so that I can discuss them with Jordan:
- Test

Questions that I might need answers to from Marek:
- `ASYNC_PLAYER` vs `PLAYER` for multi-agent, looks like we should use `ASYNC_PLAYER`

### MARL+HF
I need to spend at least an hour today working on how to use an LLM for preference labeling in SMAC. What literature exists?
