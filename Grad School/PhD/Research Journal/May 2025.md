## May 24, 2025
- Researched potential candidates for HF applied to MARL, best candidate thus far is [[MADT]], uses a similar structure to existing work in RLHF
- Need to figure out how the feedback should be structured for multi-agent, probably will depend on how the data itself is structured
- Design a very small experiment, that can use the feedback to improve performance of that agent

## May 25, 2025
- install of MADT progressing need to fix mujoco-py install error by adding to zshrc:
	- `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/marktrovinger/.mujoco/mujoco200/bin`
## May 26, 2025
- fixed (I think), the MuJoCo problem mentioned yesterday
- fixed any errors that cropped up during training, unzipped the the data files, need to figure out how the HF part would work in this case
## May 27, 2025
- started presentation document, outlined main sections
- dataset methodology:
	- MAPPO, DecPOMDP: `which owns local observations and available actions for each agent`, figure out what this means
		- Detailed in the SMAC paper
	- 3m contains 62,528 samples; probably from the beginning of the training run, need to confirm, possibly with authors
- Meeting scheduled for Friday, May 30 at 1:00PM Eastern with Dr. Sun
## May 28, 2025
- continued with presentation, need to further solidify the approach taken

## May 29, 2025
- reading deep Coach paper, looking at methodology, if applicable, add to presentation, find similar papers
	- this approach won't work, need to look at other papers, probably something like policy shaping or imitation learning
- formulate feedback as guidance or reward augmentation?

## May 30, 2025
- meeting with Dr. Sun, see [[May 30, 2025 - Dr. Sun]] notes for more
- started reproduction run on MADT
	- 