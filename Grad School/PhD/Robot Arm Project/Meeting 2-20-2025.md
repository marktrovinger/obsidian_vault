# Pre-meeting
- Q-learning progress
	- Convergence problems
		- Stuck in certain situations
	- Timesteps needed to complete subtasks leads to very long (relatively) training runs, when they complete
	- Still very buggy, seems to be a env/wrapper issue, chasing those down
	- Needs to run headless, have to figure that out
	- Reward plotting needs added
	- State: bit string, actions: discrete actions
	- 