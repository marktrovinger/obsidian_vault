## Update
- $Q$-learning results:
	- Uses hyper-parameters from Q-learning for TSP paper/post
	- Not as good as pure deterministic, error scales with task complexity, success rate is lower than we would like:
		- Pick and Place: -78, 0.78
		- Single Layer Tower: -135, 0.61
		- Two layer Tower: -255, 0.43
		- Three layer Tower: -350 (sometimes truncates) 0.36
		- Wall: doesn't converge
	- Fairly brittle, requires special handling of certain error cases
		- Robot can get stuck in un-recoverable poses, which crashes the training run
		- Handling these special cases means that we can't use off-the-shelf DRL implementations, without fixing underlying cause
		- False positives:
			- The order of block placement is not tracked (and probably should be) so it will "succeed" without actually succeeding
	- Reward structure in paper isn't quite correct:
		- Should be sum of the timesteps of individual tasks, not sum of timesteps overall:
			- $\sum subtask$  
		- Instant opening and closing of gripper isn't accurate with real-world, we may need to have it sleep for the correct number of timesteps given simulation speed

## Meeting Notes
- Leo: working on tactile, still having issues with the topic not being refreshed correctly
	- Tried using toothpaste and other circular objects
	- Learning using MPC is working, without GPU is very slow
- Submit a clean version for the camera ready
	- Use blue for any changes
	- Add to the rebuttal doc on Google Drive
		- Add the corrected equation, along with any additional clarification
		- Add the hyperparameters from the TSP paper using Q-learning
		- Add the hyperparameters from HER for DDPG, etc.
		- Add ideas for scaling the work to larger real-world tasks