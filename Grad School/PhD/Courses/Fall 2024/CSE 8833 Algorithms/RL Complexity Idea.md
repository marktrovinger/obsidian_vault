- Possible algorithms:
	- DP
		- Policy Iteration
		- Value Iteration
	- [[TD-Learning]]
		- SARSA (on policy)
		-  $Q$-learning (off-policy TD)
			- $O(d)$  
	-  [[Monte Carlo Methods]]
		- On/Off policy
		- averaging sample returns
- Experimental setup:
	- FrozenLake
		- Larger and larger sizes of map
		- state transition probability (slippery vs not)

## Algorithms to Include
- DP approaches
- Monte Carlo methods
- TD-learning

## RL History
- "optimal return function", later became known as dynamic programming
- discrete stochastic version of optimal control known as MDPs
- "curse of dimensionality"


## Resources
- HF course, see if there code implementations for MC and TD
	- using those, run experiments as detailed above
