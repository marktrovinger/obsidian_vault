## Pre-meeting
- Better understanding of both MARL and RLHF!
- Papers of interest: ABC, QT-Opt, Deep TAMER, RLHF for Robotics (reward sketch paper)
- Continuing to read, nailing down an env?
- Code executed:
	- CPPO, IsaacLab (MARL w/Shadowhand), Atari using RLHF, SMAC
- Compute requirements:
	- Full LLMs are pretty beefy computationally, what is the smallest model we can get away with using? Should it even be an LLM?
- CTDE, CTCE, DTDE
- Multi-agent DT, reading papers about this
	- TODO: Multi-agent DT papers
- PBT, worthwhile to look at?
- Newer works for DT credit assignment
- What does HF look like?
	- Preference based 
	- Short clips

# RLC Workshop Proposal
- How can we design RL agents to operate in vast, partially-observable open-world environments and engage in long-horizon tasks with sparse rewards?
	- Prior work in PO envs using DT, namely Doom
	- ABC approach could be applied to further